#!/usr/bin/env python

import argparse
from contextlib import closing
import logging
import os
import sys
from threading import Thread

import tornado.httpserver
import tornado.ioloop

from grouper.app import Application
from grouper.database import DbRefreshThread
from grouper.error_reporting import get_sentry_client, SentryProxy, setup_signal_handlers
import grouper.fe
from grouper.fe.routes import HANDLERS
from grouper.fe.settings import Settings, settings
from grouper.fe.template_util import get_template_env
from grouper.graph import Graph
from grouper.models.base.session import get_db_engine, Session
from grouper.plugin import get_plugins, load_plugins
from grouper.settings import default_settings_path
from grouper.setup import parse_args, setup_logging
from grouper.util import get_loglevel, get_database_url


def get_application(db_session_cls, settings, sentry_client, deployment_name):
    # type: (Session, Settings, SentryProxy, str) -> Application
    tornado_settings = {
        "static_path": os.path.join(os.path.dirname(grouper.fe.__file__), "static"),
        "debug": settings.debug,
        "xsrf_cookies": True,
    }

    my_settings = {
        "db_session": Session,
        "template_env": get_template_env(deployment_name=deployment_name),
    }

    application = Application(HANDLERS, my_settings=my_settings, **tornado_settings)
    application.sentry_client = sentry_client

    return application


def main(args, sentry_client):
    # type: (argparse.Namespace, SentryProxy) -> None

    log_level = logging.getLevelName(logging.getLogger().level)
    logging.info("begin. log_level={}".format(log_level))

    assert not (settings.debug and settings.num_processes > 1), \
            "debug mode does not support multiple processes"

    if settings.plugin_dir:
        assert os.path.exists(settings.plugin_dir), "Plugin directory does not exist"
        load_plugins(settings.plugin_dir, service_name="grouper_fe")

    # setup database
    logging.debug("configure database session")
    Session.configure(bind=get_db_engine(get_database_url(settings)))

    application = get_application(Session, settings, sentry_client, args.deployment_name)

    address = args.address or settings.address
    port = args.port or settings.port

    ssl_contexts = list(filter(bool, p.get_ssl_context() for p in get_plugins()))
    assert len(ssl_contexts) <=1, "Plugins returned more than one ssl.SSLContext!"

    logging.info(
        "Starting application server with %d processes on port %d",
        settings.num_processes, port
    )
    server = tornado.httpserver.HTTPServer(
            application,
            ssl_options=next(ssl_contexts, None)
    )
    server.bind(port, address=address)
    # When using multiple processes, the forking happens here
    server.start(settings.num_processes)

    # Create the Graph and start the config / graph update threads post fork to ensure each
    # process gets updated.

    settings.start_config_thread(args.config, "fe")

    with closing(Session()) as session:
        graph = Graph()
        graph.update_from_db(session)

    refresher = DbRefreshThread(settings, graph, settings.refresh_interval, sentry_client)
    refresher.daemon = True
    refresher.start()

    try:
        tornado.ioloop.IOLoop.instance().start()
    except KeyboardInterrupt:
        tornado.ioloop.IOLoop.instance().stop()
    finally:
        print "Bye"


if __name__ == "__main__":
    setup_signal_handlers()
    try:
        # get arguments
        parser = argparse.ArgumentParser(description="Grouper Web Server.")
        args = parse_args(parser, default_settings_path())

        # load settings
        settings.update_from_config(args.config, "fe")

        # setup logging
        setup_logging(args, settings.log_format)

        # setup sentry
        sentry_client = get_sentry_client(settings.sentry_dsn)
    except:
        logging.exception('uncaught exception in startup')
        sys.exit(1)

    try:
        main(args, sentry_client)
    except:
        sentry_client.captureException()
    finally:
        logging.info("end")
